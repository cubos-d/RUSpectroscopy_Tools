= First attempts of solving the inverse problem<chap:failure>

In this chapter, we explore the initial attempts to solve the inverse problem. First, we describe how synthetic data was generated by uniformly sampling the bulk modulus (K), shear modulus (G), and the dimensions of the sample within fixed ranges. For each generated entry, the corresponding resonance frequencies were computed by solving the forward problem. This dataset was then used to train machine learning models aimed at predicting the bulk and shear moduli of an isotropic solid.

Next, we present the exploratory data analysis (EDA) carried out to understand the distribution of the frequencies and identify possible transformations that could improve model performance. Despite these efforts, both a linear regression model and a random forest model failed to yield satisfactory results. This outcome highlights the complexity of the inverse problem, even in the simplest case of isotropic materials, and demonstrates the need for further preprocessing and more sophisticated modeling approaches — which will be addressed in @chap:transformations.
#v(1cm)

== Linear model

To construct the dataset used in the initial modeling attempts, combinatorial and random approaches were employed. In the combinatorial approach range of values was defined for the bulk modulus $K$, shear modulus $G$, density $rho$ and the sample dimensions $L_x$, $L_y$, and $L_z$. Within each range, a finite set of uniformly spaced values was selected, and the Cartesian product of all these sets was computed. This procedure generated all possible combinations of $K$, $G$, $rho$, and the geometric parameters for each shape. 196608 data entries were generated with the combinatorial approach.  In the random approach the data was simply generated uniformly distributed within each range. In random approach 229376 data entries were generated. The following table shows the ranges of each value: 

#figure(
 table(
   columns: 3,
   [*Variable*], [min], [max],
   [*$K$*], [0.3 $"Tdyn"/"cm"^2$], [5.6 $"Tdyn"/"cm"^2$],
   [*$G$*], [0.3 $"Tdyn"/"cm"^2$], [5.6 $"Tdyn"/"cm"^2$],
   [*$rho$*], [0.2 $g/"cm"^2$], [10 $g/"cm"^2$],
   [*$L_x$*], [0.1 $"cm"$], [1 $"cm"$],
   [*$L_y$*], [0.1 $"cm"$], [1 $"cm"$],
   [*$L_z$*], [0.1 $"cm"$], [1 $"cm"$],


 ),
 caption: [Limits of the ranges where the data was generated.]
)<table:feat_ranges>

The considered shapes were parallelepiped, cylinder, ellipsoid, cone, pyramid and tetrahedron. Initially, the goal was to solve the inverse problem regardless of the sample’s shape. However, as will be discussed in @chap:transformations, it is more practical to begin by solving the problem for one shape at a time.

=== Exploratory data analysis

A histogram of the first resonance frequency $omega_0$ was plotted using a logarithmic scale on the y-axis, and is shown in @fig:omega_distribution. This visualization helps assess the distribution shape of $omega_0$ across the dataset. The use of a log scale suggests the frequency values span multiple orders of magnitude which suggests that the features referring to the frequencies should be, in principle, their logarithms. 

#figure(
  image("../images/omega_distribution.png", width: 75%),
  caption: [Distribution of the first frequency.]
)<fig:omega_distribution>

To check if the logarithm transformation on the frequencies will be useful in building a model, let's see the distribution of $log(omega_0)$ in @fig:normal_w. 

#figure(
  image("../images/omega_log_distribution.png", width: 75%),
  caption: [Distribution of $log(omega_0)$] 
)<fig:normal_w>

It can be seen in @fig:normal_w that the logarithm of the first frequency follows something similar to a normal distribution. This confirms that the logarithm transformation was an adequate measure, since is very hard for a model to fit data distributed in different orders of magnitude. In other words the data of the frequencies is highly skewed, which leads to not normal distributed residuals, which is a requisite in a linear regression #cite(<Williams_2013>). On the other hand feeding normal distributed data to a linear model, even if is not mandatory, is adequate, because this more likely yield to normal distributed residuals. To check how similar the distribution of $log(omega_0)$ is to a normal distribution, a QQ plot that compared the quantiles of $log(omega_0)$ to the quantiles or a variable following normal distribution is shown in @fig:qq_plot.   

#figure(
  image("../images/qq_plot.png", width: 75%),
  caption: [QQ-plot for a normal distribution of the logarithms of the first frequency. The red line shows the quantiles of a variable following a normal distribution and the blue line indicates the quantiles of $log(omega_0)$]
)<fig:qq_plot>

We can see in @fig:qq_plot that the ordered values of $log(omega_0)$ are close to its theoretical quantiles. This confirms that $log(omega_0)$ distributes close to normal distribution.

Another assumption made when fitting linear models is the lack of perfect collinearity. This means that the features don't have a linear relation between them meaning that they are linearly independent. When two or more variables have linear dependency, it is a problem, because this implies that two or more variables are holding the same information, which makes one or more of them redundant, and thus useless for predicting the target. To measure the lineal dependence between features we have the Pearson product-moment correlation coefficient, which is given by the following expression #cite(<Raschka_2022>): 

$ r_(i l) = (sum_(j = 1)^(N_D)(x_(i j) - dash(x_i))(x_(l j) - dash(x_l)))/(sqrt(sum_(j=1)^(N_D) (x_(i j) - dash(x_i))^2 ) sqrt(sum_(j = 1)^(N_D)(x_(l j) - dash(x_l))^2)), $<eq:Pearson_correlation>

where, $r_(i l)$ is the Pearson product-moment correlation coefficient between the feature $i$ and the feature $l$, $N_D$ is the number of data entries, $x_(i j)$ is the value of the feature $i$ in the data entry $j$ and $dash(x_i)$ is the mean value of the feature $i$.

When the value of $r_(i l)$ is very close to -1 or 1, that means that the pair of variables are heavily correlated, either with positive or negative correlation. On the other hand a value of $r_(i l)$ close to 0 means that the pair of variables have little to no linear correlation, which means high degree of independence between the pair of variables. Every value of $r_(i l)$ between every possible pair of features can be condensed inside the correlation matrix. Observing this matrix lets us to check which variables are independent and which ones can be removed because they are not sufficiently independent, as they have values of $r_(i l)$ close to -1 or 1. @fig:corr_matrix1 shows the correlation matrix for the features of the present problem.   

#figure(
  image("../images/corr_matrix1.png", width: 75%),
  caption: [Correlation matrix between variables including features and targets. Here dx, dy and dx are $L_x$, $L_y$ and $L_z$ respectively, and w0 is $omega_0^2$, w1 is $omega_1^2$, and so on.]
)<fig:corr_matrix1>

We can see in @fig:corr_matrix1, that the features are heavily correlated. This indicates that the problem is a very complex one, because the frequencies, hold almost the same information, as they have $r_(i l)$ values between them very close to 1. This reinforces the idea that variable modifications must be done before feeding the features to a linear regression model or any other model.

The estimation of $lambda$ coefficient in Box-Cox transformation will allow us to find transformations where the transformed data has near normal distribution. Even if this is not necessary, this may lead to normal distributed residuals #cite(<Sakia_1992>). The Box-Cox transformation is given by the following expression #cite(<Sakia_1992>):

$ "BC"(lambda) = (y^lambda - 1)/lambda. $

The estimated parameter $lambda$ of Box-Cox transformation for $K$ and $G$ variables were 0.7 and 0.67 for $rho$. This suggests that for $K$, $G$ and $rho$ variables a square root transformation should be performed. These transformations make sense since the compressional ($v_K$) and shear wave speeds ($v_G$) depend on the square root of their respective modulus and the density: 

$ v_K = sqrt(K/rho), v_G = sqrt(G/rho). $

With all the transformations mentioned the targets and the features were as follows: 

- *Features*: $L_x$, $L_y$, $L_z$, $sqrt(rho)$ and the logarithms of the first 10 frequencies $log(omega_n)$.
- *Targets*: $sqrt(K)$ and $sqrt(G)$.

The dataset generated with the random approach transformed as mentioned before was used to fit an ordinary linear regression. The results are shown in the following table [Aquí ya estamos viendo el cuadervo de cv_linearregress. La primera estadística es de la regresión lineal simple]:  
