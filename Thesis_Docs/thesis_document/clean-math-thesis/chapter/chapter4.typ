= First attempts of to solve the inverse problem<chap:failure>

In this chapter, we explore the initial attempt to solve the inverse problem. First, we describe how synthetic data was generated by uniformly sampling the bulk modulus (K), shear modulus (G), and the dimensions of the sample within experimentally fixed ranges. For each generated entry, the corresponding resonance frequencies were computed by solving the forward problem. This dataset was then used to train machine learning models aimed at predicting the bulk and shear moduli of an isotropic solid.

Next, we present the exploratory data analysis (EDA) carried out to understand the distribution of the frequencies and identify possible transformations that could improve model performance. Despite these efforts, both a linear regression model and a random forest model failed to yield satisfactory results, according to what's required in a RUS experiment. This outcome highlights the complexity of the inverse problem, even in the simplest case of isotropic materials, and demonstrates the need for further preprocessing and more sophisticated modeling approaches — which will be addressed in @chap:transformations.
#v(1cm)

== The Inverse problem<sec:inv_4>

As discussed in @chap:forward, the forward problem consists of computing the resonance frequencies of a solid given its elastic constants, geometry, and mass (or density). In contrast, the focus of this chapter is the inverse problem, which involves doing the opposite: determining the elastic constants from the measured frequencies, along with the known geometry and mass (or density) of the sample.

A schematic summary of this formulation is shown in @fig:Inverse_simplified_diagram_again.
//Photo of the inverse problem simplified AGAIN!!!
#figure(
  image("../images/Inverse_diagram_basic.png", width: 60%),
  caption: [Flow diagram representing the formulation of the inverse problem.]
)<fig:Inverse_simplified_diagram_again>

== Data generation

To construct the dataset used in the initial modeling attempts, both combinatorial and random sampling strategies were employed.

In the combinatorial approach, a range of values was defined for the bulk modulus $K$, shear modulus $G$, density $rho$, and the sample dimensions $L_x$, $L_y$, and $L_z$. Within each range, a finite set of uniformly spaced values was selected, and the Cartesian product of all these sets was computed. This procedure generated all possible combinations of $K$, $G$, $rho$, and the geometric parameters for each shape. A total of 196608 data entries were generated using this method.

In the random approach, data points were generated by uniform sampling within the same predefined ranges. This approach produced 229376 data entries.

The table below shows the minimum and maximum values used for each variable during the data generation process:

#figure(
 table(
   columns: 3,
   [*Variable*], [min], [max],
   [*$K$*], [0.3 $"Tdyn"/"cm"^2$], [5.6 $"Tdyn"/"cm"^2$],
   [*$G$*], [0.3 $"Tdyn"/"cm"^2$], [5.6 $"Tdyn"/"cm"^2$],
   [*$rho$*], [0.2 $g/"cm"^2$], [10 $g/"cm"^2$],
   [*$L_x$*], [0.1 $"cm"$], [1 $"cm"$],
   [*$L_y$*], [0.1 $"cm"$], [1 $"cm"$],
   [*$L_z$*], [0.1 $"cm"$], [1 $"cm"$],


 ),
 caption: [Limits of the ranges where the data was generated.]
)<table:feat_ranges>

The considered shapes were parallelepiped, cylinder, ellipsoid, cone, pyramid and tetrahedron. Initially, the goal was to solve the inverse problem regardless of the sample’s shape. However, as will be discussed in @chap:transformations, it is more practical to begin by solving the problem for one shape at a time.

== Exploratory data analysis

Using the data generated with the combinatorial approach, a plot of the distribution of first resonance frequency $omega_0$ was made, using ```python sns.histplot()``` #cite(<seaborn>) and is shown in @fig:omega_distribution. This visualization helps assess the distribution shape of $omega_0$ across the dataset. The shape of @fig:omega_distribution reveals that the values of $omega_0$ span several orders of magnitude, indicating a highly skewed distribution. In such cases, applying a logarithmic transformation is a common practice in data preprocessing, as it helps normalize the distribution, reduce skewness, and compress outliers, making patterns more discernible to both statistical models and machine learning algorithms. Therefore, it is reasonable to consider using the logarithms of the frequencies as input features, rather than the raw frequency values. 

#figure(
  image("../images/omega_distribution.png", width: 65%),
  caption: [Distribution of the first frequency.]
)<fig:omega_distribution>

To check if the logarithm transformation on the frequencies will be useful in building a model, let's see the distribution of $log(omega_0)$ in @fig:normal_w. 

#figure(
  image("../images/omega_log_distribution.png", width: 65%),
  caption: [Distribution of $log(omega_0)$] 
)<fig:normal_w>

It can be seen in @fig:normal_w that the logarithm of the first frequency, $log(omega_0)$, follows a distribution that is approximately normal. This transformation is widely used in machine learning and statistical modeling to handle skewed data and reduce the influence of outliers. In the original distribution (shown in @fig:omega_distribution), frequencies above 2 MHz are heavily underrepresented, resulting in a long tail and poor balance across the range of values.

Logarithmic transformations compress this dynamic range, improve symmetry, and help models — particularly linear and gradient-based models — perform better by stabilizing variance and enhancing numerical robustness. Transforming features in a way the outliers are reduced improves learning outcomes #cite(<Geron_2019>). We can see in @fig:box_plot_shapes that the $omega_0$ has a lot of outliers.  Additionally, Kuhn & Johnson emphasize the importance of addressing skewness and scale before model training #cite(<Kuhn_2013>).

A Q–Q plot comparing $log(omega_0)$ to a theoretical normal distribution is shown in @fig:qq_plot, confirming the improved behavior after transformation.

#figure(
  image("../images/qq_plot.png", width: 65%),
  caption: [QQ-plot for a normal distribution of the logarithms of the first frequency. The red line shows the quantiles of a variable following a normal distribution and the blue line indicates the quantiles of $log(omega_0)$]
)<fig:qq_plot>

The box plot shown in @fig:box_plot_shapes further confirms that applying a logarithmic transformation to the frequencies is beneficial. For instance, in the case of the parallelepiped shape, the interquartile range of $omega_0$ lies approximately between 10 kHz and 0.2 MHz—well outside the ultrasound frequency regime. Moreover, as seen on the right side of @fig:box_plot_shapes, most frequencies that fall within the ultrasound range appear as outliers. This highlights the skewed nature of the raw data and provides additional justification for transforming the frequency values prior to modeling.

#figure(
  image("../images/box_plot_shapes.png", width: 90%),
  caption: [Box plot showing the quantiles of the first frequency for each shape. Both are the same box plots, but the one in the right has the vertical axis in logarithmic scale. The dots in the right box-plot represent the outliers.]
)<fig:box_plot_shapes>

Another assumption made when fitting machine learning models is the lack of perfect collinearity. This means that the features don't have a linear relation between them, meaning that they are linearly independent. When two or more variables have linear dependency, it is a problem, because this implies that two or more variables are holding the information, which makes one or more of them redundant, and thus useless for predicting the target. To measure the linear dependence between features we have the Pearson product-moment correlation coefficient, which is given by the following expression #cite(<Raschka_2022>): 

$ r_(i l) = (sum_(j = 1)^(N_D)(x_(i j) - dash(x)_i)(x_(l j) - dash(x)_l))/(sqrt(sum_(j=1)^(N_D) (x_(i j) - dash(x)_i)^2 ) sqrt(sum_(j = 1)^(N_D)(x_(l j) - dash(x)_l)^2)), $<eq:Pearson_correlation>

where, $r_(i l)$ is the Pearson product-moment correlation coefficient between the feature $i$ and the feature $l$, $N_D$ is the number of data entries, $x_(i j)$ is the value of the feature $i$ in the data entry $j$ and $dash(x)_i$ is the mean value of the feature $i$.

When the value of $r_(i l)$ is very close to -1 or 1, that means that the pair of variables are heavily correlated, either with positive or negative correlation. On the other hand, a value of $r_(i l)$ close to 0 means that the pair of variables have little to no linear correlation, which means a high degree of independence between the pair of variables. Every value of $r_(i l)$ between every possible pair of features can be condensed in the correlation matrix. Observing this matrix let us to check which variables are independent and which ones can be removed because they are not sufficiently independent, as they have values of $r_(i l)$ close to -1 or 1. @fig:corr_matrix1 shows the correlation matrix for the features and targets of the isotropic solid.   

#figure(
  image("../images/corr_matrix1.png", width: 75%),
  caption: [Correlation matrix between variables including features and targets. Here dx, dy and dx are $L_x$, $L_y$ and $L_z$ respectively, and w0 is $omega_0^2$, w1 is $omega_1^2$, and so on.]
)<fig:corr_matrix1>

As shown in @fig:corr_matrix1, the resonance frequencies are heavily correlated—they contain largely redundant information, with correlation coefficients $r_(i l)$ between them very close to 1. However, we cannot simply discard most of the frequencies (e.g., retain only $omega_0$) just because they are correlated. This is because predicting two target variables requires multiple independent inputs to capture sufficient variation. This highlights the need for feature engineering and variable transformations before using the frequencies as input to a linear regression model or any other machine learning algorithm.

One method for identifying effective transformations is the Box–Cox transformation, which estimates a parameter $lambda$ that can make the transformed variable approximately normally distributed. While normality is not strictly required, it can improve model performance by producing more stable variance and more normally distributed residuals #cite(<Sakia_1992>). The Box–Cox transformation is defined by the following expression #cite(<Sakia_1992>):

$ "BC"(lambda) = (y^lambda - 1)/lambda. $

The estimated Box–Cox transformation parameters ($lambda$) for the variables $K$, $G$, were approximately 0.7, and for and $rho$ were approximately 0.67. These values suggest that a square root transformation would be appropriate for these variables. In contrast, the estimated $lambda$ values for all the resonance frequencies were below zero, indicating that a logarithmic transformation is more suitable for the frequency variables—consistent with the analysis previously discussed. @table:box_cox_table shows the transformations to perform according to the estimated value of $lambda$.

#figure(
  table(
    columns: 2,
    [*$lambda$*], [*Transformation*],
    [$lt.eq 0$], [logarithm],
    [$0.5$], [square root],
    [$>1$], [power],
  ),
  caption: [Transformations according to $lambda$ parameter in Box-Cox.]
)<table:box_cox_table>

These transformations make sense since the compressional ($v_K$) and shear ($v_G$) wave speeds depend on the square root of their respective modulus and the density, as shown in @eq:wave_speeds. Those speeds are related with related with their respective frequencies and the relation volume/area ("r") of the sample the following way: $v = omega r$. This gives us the hint that the frequencies depend on the geometry of the sample. 

$ v_K = sqrt(K/rho); " "v_G = sqrt(G/rho). $<eq:wave_speeds>

After applying the transformations discussed above, the input features and targets used for the modeling were redefined as follows:

- *Features*: the sample dimensions $L_x$, $L_y$, and $L_z$; the square root of the density, $rho$; and the logarithms of the first 10 resonance frequencies, $log(omega_n)$.

- *Targets*: the square roots of the elastic constants, $K$ and $G$.

== Fitting a linear model

The dataset, generated with the combinatorial approach and transformed as mentioned before was used to fit an ordinary linear regression, using statsmodels library #cite(<statsmodels>). In a linear regression a target $y$ is fitted in terms of some features $x_i$ the following way: 

$ y = sum_(i = 0)^(N_F) A_i x_i + A_0, $

where $N_F$ is the number of features and $A_i$ are the coefficients. For example, a model that fits $sqrt(K)$ in terms of the features mentioned above can be written as: 

$ sqrt(K) = A_0 + A_1 log(omega_1) + A_2 log(omega_2) + dots. $

The results, of the prediction power of the model for $K$ target, in the linear regression, are shown in the following table:

#figure(
  table(
    columns: 3,
    [], [*Train*], [*Test*],
    [*RMSE*], [0.650 $"Tdyn"/"cm"^2$], [0.651 $"Tdyn"/"cm"^2$], 
    [*MAE*], [0.532 $"Tdyn"/"cm"^2$], [0.534 $"Tdyn"/"cm"^2$],
    [*MAPE*], [40%], [40%],
    [*$R^2$*], [0.86], [0.86]
  ),
  caption: [Performance metrics for $K$ prediction in the first linear model trained.]
)<table:resultados_failure_1>

Before analyzing @table:resultados_failure_1 lets define the metrics shown there. RMSE stands for Root Mean Squared Error, and is defined the following way:

$ "RMSE" = sqrt(1/N_D sum_(j = 1)^(N_D)(y_j - hat(y)_j)^2), $<eq:RMSE_definition>

where $N_D$ is the number of data entries, $y_j$ is the value of the target and $hat(y)_j$ is the prediction of the value of the target. This is no other than the objective function to be minimized when fitting a regression model. To have an idea of the error we could expect when using the model we have the MAE, which stands for Mean Absolute Error, and is defined the following way:

$ "MAE" = 1/N_D sum_(j = 1)^(N_D) abs(y_j - hat(y)_j). $<eq:MAE_DEFINITIONS>

Other metric that will give us the expected percentage error is the MAPE, which stands for Mean Absolute Percentage Error, is defined the following way:

$ "MAPE" = 1/N_D sum_(j=1)^(N_D) abs((y_j - hat(y)_j)/y_j). $<eq:MAPE_definition>

The last metric shown in @table:resultados_failure_1 is the coefficient of determination $R^2$ which gives us an idea whether the data is behaving like the model intends to. In this case it tells us if the data has linear behavior. 

In the case of the first linear model, we could expect an error of 0.534 $"Tdyn"/"cm"^2$ or 53.4 $"GPa"$ for using the model. This is a huge error considering that, for example, this is approximately the difference between the constant $C_11$ of silver and the constant $C_11$ of NaCl, as we can see in @apx:cubic_constants. Also, the MAPE is telling us that we can expect an error of 40% when predicting a constant, which is not experimentally tolerable. On the other hand, the $R^2$ values suggest us that $sqrt(K)$ has near lineal dependence respect to $log(omega_n)$. @fig:statsmodel_results shows the hypothesis tests done to every feature in the linear regression model.

#figure(
  image("../images/statsmodels_results.png", width: 65%),
  caption: [Summary of the model prompted by statsmodels.] 
)<fig:statsmodel_results>

We can see that, all the p-values reported in @fig:statsmodel_results referring to the frequencies are below 0.05. This indicates that statistically every constant of each frequency is important and is non zero. This tells us that every constant is an important feature an all of them should be included in the model. To conclude the discussion of the linear model predicting $K$, @fig:residuals shows the distribution of the residuals. We can see there that the residuals follow a near normal distribution, which means that the model complies with the homoscedasticity principle. 

#figure(
  image("../images/residuals.png", width: 65%),
  caption: [Distribution of the residuals in the first linear model predicting $K$.]
)<fig:residuals>

Another independent linear model was trained to predict $G$ with the combinatorial dataset. The results of the prediction power of this model are shown in the following table: 

#figure(
  table(
    columns: 3,
    [], [*Train*], [*Test*],
    [*RMSE*], [0.620 $"Tdyn"/"cm"^2$], [0.625 $"Tdyn"/"cm"^2$], 
    [*MAE*], [0.510 $"Tdyn"/"cm"^2$], [0.514 $"Tdyn"/"cm"^2$],
    [*MAPE*], [37%], [37%],
    [*$R^2$*], [0.90], [0.86]
  ),
  caption: [Performance metrics for $G$ prediction in the first linear model trained.]
)<table:resultados_failure_G>

We can see that the metric of the model predicting $G$ are very similar to the metric of the model predicting $K$. The analysis done above to the linear model predicting $K$ also applies here. The errors of the model predicting $G$ are slightly smaller than the ones in the model predicting $K$. This slight improvement may happen because it is expected that $G$ has more dependence on the lower frequencies, since a shear deformation on any material, usually, requires less energy than a volume deformation. 

The results above suggest that a more complex model (a model which takes account no linearities and has more parameters), which includes the geometric features ($L_x$, $L_y$ and $L_z$), is needed to fit $K$ and $G$. Other models of polynomial regression, which included interaction terms, were trained. Some of those models were trained with polynomial features up to degree 5

/*
== Other models trained

Several polynomial models and Random Forest models were trained with the data generated with the random approach. All the models showed high values of RMSE, MAE and MAPE metrics even in the train data. This means that some of those models weren't even able to overfit the data. Other models just displayed high values of RMSE, MAE and MAPE metrics on the test data. The results of the model with the lowest values of those metrics is shown in @table:resultados_failure_2. This was a random forest model with 200 estimators and other parameters left by default, trained with polynomial features of degree 3 and "Yeo-Johnson" power transformation. The details about the parameters of a Random Forest and the "Yeo-Johnson" transform can be found in the scikit-learn documentation #cite(<scikit-learn>). Also the training and the testing of this model was performed only using the data of parallelepiped shapes. 

#figure(
  table(
    columns: 3,
    [], [*Train*], [*Test*],
    [*RMSE*], [0.199 $"Tdyn"/"cm"^2$], [0.527 $"Tdyn"/"cm"^2$],
    [*MAE*], [0.125 $"Tdyn"/"cm"^2$], [0.337 $"Tdyn"/"cm"^2$],
    [*MAPE*], [7.58%], [23.21%]
  ),
  caption: [Performance metric for the best model (lowest RMSE, MAE and MAPE matrics) trained with the approach presented in the present chapter.]
)<table:resultados_failure_2>

The test metrics given by this model are poor. For example, MAPE metric is telling us that the expected percentage error obtained from using the model is 23.21%. Also, the difference between the train and test metrics in @table:resultados_failure_2 shows that the model is overfitting. This means that solving the inverse problem the way it is being solved in the present chapter needs a very complex model, with a huge number of training parameters. Also more data is needed in order to solve such model to avoid overfitting. There is another way of solving the inverse problem without needing complex models with lots of parameters, at least for the isotropic case as shown in @chap:inverse_problem. 

Also the approach of solving the inverse problem in the present chapter is very limited. It only intends to solve the inverse problem for samples whose characteristics, like its $K$, $G$ and dimension values, are within the ranges exposed in @table:feat_ranges. It is possible to have a non limited approach with a solution of the inverse problem for samples regardless of its dimensions, size or elastic constants scale. @chap:transformations will explore this approach right now. 

*/

