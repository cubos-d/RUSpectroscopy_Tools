= First attempts of solving the inverse problem<chap:failure>

In this chapter, we explore the initial attempts to solve the inverse problem. First, we describe how synthetic data was generated by uniformly sampling the bulk modulus (K), shear modulus (G), and the dimensions of the sample within fixed ranges. For each generated entry, the corresponding resonance frequencies were computed by solving the forward problem. This dataset was then used to train machine learning models aimed at predicting the bulk and shear moduli of an isotropic solid.

Next, we present the exploratory data analysis (EDA) carried out to understand the distribution of the frequencies and identify possible transformations that could improve model performance. Despite these efforts, both a linear regression model and a random forest model failed to yield satisfactory results. This outcome highlights the complexity of the inverse problem, even in the simplest case of isotropic materials, and demonstrates the need for further preprocessing and more sophisticated modeling approaches — which will be addressed in @chap:transformations.
#v(1cm)


== Data generation

To construct the dataset used in the initial modeling attempts, combinatorial and random approaches were employed. In the combinatorial approach range of values was defined for the bulk modulus $K$, shear modulus $G$, density $rho$ and the sample dimensions $L_x$, $L_y$, and $L_z$. Within each range, a finite set of uniformly spaced values was selected, and the Cartesian product of all these sets was computed. This procedure generated all possible combinations of $K$, $G$, $rho$, and the geometric parameters for each shape. 196608 data entries were generated with the combinatorial approach.  In the random approach the data was simply generated uniformly distributed within each range. In random approach 229376 data entries were generated. The following table shows the ranges of each value: 

#figure(
 table(
   columns: 3,
   [*Variable*], [min], [max],
   [*$K$*], [0.3 $"Tdyn"/"cm"^2$], [5.6 $"Tdyn"/"cm"^2$],
   [*$G$*], [0.3 $"Tdyn"/"cm"^2$], [5.6 $"Tdyn"/"cm"^2$],
   [*$rho$*], [0.2 $g/"cm"^2$], [10 $g/"cm"^2$],
   [*$L_x$*], [0.1 $"cm"$], [1 $"cm"$],
   [*$L_y$*], [0.1 $"cm"$], [1 $"cm"$],
   [*$L_z$*], [0.1 $"cm"$], [1 $"cm"$],


 ),
 caption: [Limits of the ranges where the data was generated.]
)<table:feat_ranges>

The considered shapes were parallelepiped, cylinder, ellipsoid, cone, pyramid and tetrahedron. Initially, the goal was to solve the inverse problem regardless of the sample’s shape. However, as will be discussed in @chap:transformations, it is more practical to begin by solving the problem for one shape at a time.

== Exploratory data analysis

Using the data generated with the combinatorial approach, a histogram of the first resonance frequency $omega_0$ was plotted using a logarithmic scale on the y-axis, and is shown in @fig:omega_distribution. This visualization helps assess the distribution shape of $omega_0$ across the dataset. The use of a log scale suggests the frequency values span multiple orders of magnitude which suggests that the features referring to the frequencies should be, in principle, their logarithms. 

#figure(
  image("../images/omega_distribution.png", width: 75%),
  caption: [Distribution of the first frequency.]
)<fig:omega_distribution>

To check if the logarithm transformation on the frequencies will be useful in building a model, let's see the distribution of $log(omega_0)$ in @fig:normal_w. 

#figure(
  image("../images/omega_log_distribution.png", width: 75%),
  caption: [Distribution of $log(omega_0)$] 
)<fig:normal_w>

It can be seen in @fig:normal_w that the logarithm of the first frequency follows something similar to a normal distribution. This confirms that the logarithm transformation was an adequate measure, since is very hard for a model to fit data distributed in different orders of magnitude. In other words the data of the frequencies is highly skewed, which leads to not normal distributed residuals, which is a requisite in a linear regression #cite(<Williams_2013>). On the other hand feeding normal distributed data to a linear model, even if is not mandatory, is adequate, because this more likely yield to normal distributed residuals. To check how similar the distribution of $log(omega_0)$ is to a normal distribution, a QQ plot that compared the quantiles of $log(omega_0)$ to the quantiles or a variable following normal distribution is shown in @fig:qq_plot.   

#figure(
  image("../images/qq_plot.png", width: 75%),
  caption: [QQ-plot for a normal distribution of the logarithms of the first frequency. The red line shows the quantiles of a variable following a normal distribution and the blue line indicates the quantiles of $log(omega_0)$]
)<fig:qq_plot>

We can see in @fig:qq_plot that the ordered values of $log(omega_0)$ are close to its theoretical quantiles. This confirms that $log(omega_0)$ distributes close to normal distribution.

Another assumption made when fitting linear models is the lack of perfect collinearity. This means that the features don't have a linear relation between them meaning that they are linearly independent. When two or more variables have linear dependency, it is a problem, because this implies that two or more variables are holding the same information, which makes one or more of them redundant, and thus useless for predicting the target. To measure the lineal dependence between features we have the Pearson product-moment correlation coefficient, which is given by the following expression #cite(<Raschka_2022>): 

$ r_(i l) = (sum_(j = 1)^(N_D)(x_(i j) - dash(x_i))(x_(l j) - dash(x_l)))/(sqrt(sum_(j=1)^(N_D) (x_(i j) - dash(x_i))^2 ) sqrt(sum_(j = 1)^(N_D)(x_(l j) - dash(x_l))^2)), $<eq:Pearson_correlation>

where, $r_(i l)$ is the Pearson product-moment correlation coefficient between the feature $i$ and the feature $l$, $N_D$ is the number of data entries, $x_(i j)$ is the value of the feature $i$ in the data entry $j$ and $dash(x_i)$ is the mean value of the feature $i$.

When the value of $r_(i l)$ is very close to -1 or 1, that means that the pair of variables are heavily correlated, either with positive or negative correlation. On the other hand a value of $r_(i l)$ close to 0 means that the pair of variables have little to no linear correlation, which means high degree of independence between the pair of variables. Every value of $r_(i l)$ between every possible pair of features can be condensed inside the correlation matrix. Observing this matrix lets us to check which variables are independent and which ones can be removed because they are not sufficiently independent, as they have values of $r_(i l)$ close to -1 or 1. @fig:corr_matrix1 shows the correlation matrix for the features of the present problem.   

#figure(
  image("../images/corr_matrix1.png", width: 75%),
  caption: [Correlation matrix between variables including features and targets. Here dx, dy and dx are $L_x$, $L_y$ and $L_z$ respectively, and w0 is $omega_0^2$, w1 is $omega_1^2$, and so on.]
)<fig:corr_matrix1>

We can see in @fig:corr_matrix1, that the features are heavily correlated. This indicates that the problem is a very complex one, because the frequencies, hold almost the same information, as they have $r_(i l)$ values between them very close to 1. This reinforces the idea that variable modifications must be done before feeding the features to a linear regression model or any other model.

The estimation of $lambda$ coefficient in Box-Cox transformation will allow us to find transformations where the transformed data has near normal distribution. Even if this is not necessary, this may lead to normal distributed residuals #cite(<Sakia_1992>). The Box-Cox transformation is given by the following expression #cite(<Sakia_1992>):

$ "BC"(lambda) = (y^lambda - 1)/lambda. $

The estimated parameter $lambda$ of Box-Cox transformation for $K$ and $G$ variables were 0.7 and 0.67 for $rho$. This suggests that for $K$, $G$ and $rho$ variables a square root transformation should be performed. These transformations make sense since the compressional ($v_K$) and shear wave speeds ($v_G$) depend on the square root of their respective modulus and the density: 

$ v_K = sqrt(K/rho), v_G = sqrt(G/rho). $

With all the transformations mentioned the targets and the features were as follows: 

- *Features*: $L_x$, $L_y$, $L_z$, $sqrt(rho)$ and the logarithms of the first 10 frequencies: $log(omega_n)$.
- *Targets*: $sqrt(K)$ and $sqrt(G)$.

== Fitting a linear model

The dataset generated with the random approach transformed as mentioned before was used to fit an ordinary linear regression, because this dataset showed better results than the model trained with combinatorial data. The results, of the prediction power of the model for $K$ target, are shown in the following table:

#figure(
  table(
    columns: 3,
    [], [*Train*], [*Test*],
    [*RMSE*], [1.511 $"Tdyn"/"cm"^2$], [1.515 $"Tdyn"/"cm"^2$], 
    [*MAE*], [1.291 $"Tdyn"/"cm"^2$], [1.296 $"Tdyn"/"cm"^2$],
    [*MAPE*], [80%], [80%]
  ),
  caption: [Performance metrics for the first linear model trained.]
)<table:resultados_failure_1>

Before analyzing @table:resultados_failure_1 lets define the metrics shown there. RMSE stands for Root Mean Squared Error, and is defined the following way:

$ "RMSE" = sqrt(1/N_D sum_(j = 1)^(N_D)(y_j - hat(y)_j)^2), $<eq:RMSE_definition>

where $N_D$ is the number of data entries, $y_j$ is the value of the target and $hat(y)_j$ is the prediction of the value of the target. This is no other than the objective function to be minimized when fitting a regression model. To have an idea of the error we could expect when using the model we have the MAE, which stands for Mean Absolute Error, and is defined the following way:

$ "MAE" = 1/N_D sum_(j = 1)^(N_D) abs(y_j - hat(y)_j). $

In the case of the first linear model, we could expect an error of 1.291 $"Tdyn"/"cm"^2$ or 129.1 $"GPa"$ for using the model. This is a huge error considering that, for example, this is approximately the difference between the constant $C_11$ of gold and the constant $C_11$ of potassium fluoride, as we can see in @apx:cubic_constants. These results suggest that a more complex model (a model which takes account no linearities and has more parameters) is needed to fit $K$ and $G$. The next model that was tried was a polynomial regression. 


== Other models trained

Several polynomial models and Random Forest models were trained with the data generated with the random approach. All the models showed high values of RMSE, MAE and MAPE metrics even in the train data. This means that some of those models weren't even able to overfit the data. Other models just displayed high values of RMSE, MAE and MAPE metrics on the test data. The results of the model with the lowest values of those metrics is shown in @table:resultados_failure_2. This was a random forest model with 200 estimators and other parameters left by default, trained with polynomial features of degree 3 and "Yeo-Johnson" power transformation. The details about the parameters of a Random Forest and the "Yeo-Johnson" transform can be found in the scikit-learn documentation #cite(<scikit-learn>). Also the training and the testing of this model was performed only using the data of parallelepiped shapes. 

#figure(
  table(
    columns: 3,
    [], [*Train*], [*Test*],
    [*RMSE*], [0.199 $"Tdyn"/"cm"^2$], [0.527 $"Tdyn"/"cm"^2$],
    [*MAE*], [0.125 $"Tdyn"/"cm"^2$], [0.337 $"Tdyn"/"cm"^2$],
    [*MAPE*], [7.58%], [23.21%]
  ),
  caption: [Performance metric for the best model (lowest RMSE, MAE and MAPE matrics) trained with the approach presented in the present chapter.]
)<table:resultados_failure_2>

The test metrics given by this model are poor. For example, MAPE metric is telling us that the expected percentage error obtained from using the model is 23.21%. Also, the difference between the train and test metrics in @table:resultados_failure_2 shows that the model is overfitting. This means that solving the inverse problem the way it is being solved in the present chapter needs a very complex model, with a huge number of training parameters. Also more data is needed in order to solve such model to avoid overfitting. There is another way of solving the inverse problem without needing complex models with lots of parameters, at least for the isotropic case as shown in @chap:inverse_problem. 

Also the approach of solving the inverse problem in the present chapter is very limited. It only intends to solve the inverse problem for samples whose characteristics, like its $K$, $G$ and dimension values, are within the ranges exposed in @table:feat_ranges. It is possible to have a non limited approach with a solution of the inverse problem for samples regardless of its dimensions, size or elastic constants scale. @chap:transformations will explore this approach right now. 
