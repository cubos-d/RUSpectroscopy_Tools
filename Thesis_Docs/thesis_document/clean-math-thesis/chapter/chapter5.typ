= Making the inverse problem a simpler one <chap:transformations>

In Chapter @chap:forward, we discussed how to obtain the resonance frequencies of a sample in resonant ultrasound spectroscopy, given its elastic constants, mass (or density), and dimensions — a process known as the forward problem. In this chapter, we turn our attention to the inverse problem, and present a set of variable transformations and techniques aimed at simplifying it. We will explain the motivation behind these transformations, their role in making the problem more tractable, and describe the specific methods applied in this work.

#v(1cm)

== Why do we need to make some tweaks before solving the inverse problem

Solving the inverse problem requires building a model or pipeline capable of predicting the elastic constants $C_11$, $C_12$, and $C_44$ from the sample’s mass (or density $rho$), dimensions ($L_x$, $L_y$ and $L_z$), and resonance frequencies ($omega_n$). To achieve this, synthetic data is generated by sampling a variety of elastic constants, densities, and dimensions, and computing their corresponding resonance frequencies using the forward problem. The resulting dataset is then used to train a machine learning model, where the frequencies, mass, and dimensions serve as input features, and the elastic constants are the target outputs.

As discussed in @chap:failure, this inverse problem is highly complex and cannot be solved directly without some form of simplification — even in the isotropic case, which is the simplest scenario and involves only two independent elastic constants. The following section outlines the reasons such simplifications and variable transformations are necessary.

- We would need to either generate data for all possible dimensions, with each dimension $L_i$ ranging, in principle, from zero to infinity — which is computationally infeasible — or restrict the data to a narrow, specific range, which would limit the model’s applicability to samples within that range. However, our goal is to develop a solution that performs well regardless of the sample’s dimensions or overall size. Ideally, the model should be valid for any parallelepiped-shaped sample.

- The elastic constants must also be generated over a certain range — but in principle, this range is unknown. While one could consult the literature to find typical values for known materials, doing so would restrict the model to predicting constants within a fixed range (such as the one shown in @table:feat_ranges), which is not the objective. Our goal is to develop a tool capable of predicting the elastic constants of unknown materials. Fortunately, physics imposes constraints on these constants to ensure the thermodynamic stability of materials. These constraints are well-described in the study by Mouhat #cite(<Mouhat_2014>). For the case of cubic solids, the applicable limits were analyzed in @section:Constant_Restrictions. These bounds will be used to design target transformations that enable the model to predict valid elastic constants. It does not matter whether an elastic constant has a value of 0.1 GPa, 10 GPa, 500 GPa, 1000 GPa, or even an extreme value; our aim is to develop a model capable of predicting any physically valid value within the thermodynamically allowed range. 

- As shown in @fig:corr_matrix1, the resonance frequencies are not fully independent. There is significant collinearity among them, meaning that many of the frequency features convey redundant information. This redundancy can hinder model performance and interpretability.

- In general, simplifying the problem leads to better results. Reducing the number of input features improves generalization and lowers computational cost. The same applies to the targets — and in our case, it is even possible to eliminate one of the target variables without losing predictive power. Simplicity also makes models easier to train, interpret, and modify.

In summary, we cannot simply train a model using data generated from forward problems by feeding it frequencies, mass, and dimensions as input features, and the elastic constants as targets. The problem must be carefully reformulated and simplified to ensure a successful solution.

== Getting eigenvalues that are independent of the size of the sample

As shown in equations @eq:Gamma_matrix_def and @eq:raw_eig_problem, the squared resonance frequencies $omega^2$ depend on the size of the sample. For example, suppose we measure the frequencies of a given sample and denote them as $omega_i^2$. If we then measure the frequencies of a second sample made from the same material and with the same shape, but scaled to twice the size (i.e., $L_(x 2)=2L_(x 1)$, $L_(y 2)=2L_(y 1)$, $L_(z 2)=2L_(z 1)$), the resulting squared frequencies will be one-fourth those of the first sample. In other words, the new frequencies will satisfy $omega_(i, "new")^2 = 1/4 omega_i^2$.

This dependence on sample size is problematic. Our objective is to predict the elastic constants of a material regardless of the sample’s size. Therefore, we must transform the raw frequencies into a form that is invariant under scaling.

Let's multiply to both sides of the equation @eq:raw_eig_problem by $V/R$, where $V$ is the volume of the sample and let 

$ R = sqrt(L_x^2 + L_y^2 + L_z^2). $<eq:R>

With this scaling equation @eq:raw_eig_problem becomes: 

 $ rho V omega^2/R arrow.l.r(Epsilon) arrow(a) = V/R arrow.l.r(Gamma) arrow(a). $<eq:eig_preparing>

Now, let's define a new matrix $arrow.l.r(Kai) = V/R arrow.l.r(Gamma)$. With this definition we arrive at a new generalized eigenvalue problem: 

$ arrow.l.r(Kai) arrow(a) = lambda arrow.l.r(Epsilon) arrow(a). $<eq:eig_final>

Here 

$ lambda = m omega^2/R, $<eq:def_lambda>

and an element of the matrix $arrow.l.r(Kai)$ is given by:  

$ Kai_(i lambda_1 mu_1 nu_1 ; k lambda_2 mu_2 nu_2) = L_(j l)/R C_(i j k l) integral_V (partial X^(lambda_1) Y^(mu_1) Z^(nu_1) )/(partial b_j) (partial X^(lambda_2) Y^(mu_2) Z^(nu_2) )/(partial b_l) d X d Y d Z, $<eq:Peso_matrix_def>

where $L_(j l) = L_(6 - j - l)$ if $j != l$. Else $L_(j l) = (L_x L_y L_z)/L_j^2$.

From equations @eq:eig_final and @eq:Peso_matrix_def, we observe that the eigenvalues $lambda$ do not depend on the absolute size of the sample. This is because $L_(j l)/R$ remains unchanged under uniform scaling of the sample and depends only on the relative proportions of $L_x$, $L_y$, and $L_z$. Consequently, the matrix $arrow.l.r(Kai)$ depends solely on these proportions and the values of the elastic constants. This implies that the eigenvalues $lambda$ are determined entirely by the elastic constants and the aspect ratio of the sample. As a result, we can eliminate the explicit influence of the sample size — originally encoded in the variables $L_x$, $L_y$, and $L_z$ — and we will show how this is accomplished in the following section.

== Defining the shape of a three dimensional (3D) sample with two parameters<sec:eta_beta_definition>

To define the aspect ratio of a solid, only two parameters are needed — for example, $L_x/L_y$ and $L_y/L_z$. However, generating data using these ratios poses a challenge: both $L_x/L_y$ and $L_y/L_z$ can take arbitrarily large values, which reintroduces the problem of working over a computationally prohibitive range.

Instead, we could use the following normalized parameters: $g_x = L_x/(L_x + L_y + L_z)$ and $g_y = L_y/(L_x + L_y + L_z)$. These values are constrained between 0 and 1, making them more practical for data generation. However, they still present a complication: not all combinations of $g_x$ and $g_y$ within [0, 1] are valid. For instance, choosing $g_x = 0.5$ and $g_y = 0.6$ is not permitted, because the sum must satisfy the condition: $g_x + g_y lt.eq 1$, since the $g$ values represent compositional fractions of $L_x + L_y + L_z$. 

Although it is possible to generate valid $g_x$ and $g_y$ values while respecting this constraint, there exists an alternative approach that simplifies the problem significantly — as we will see next.

Let us recall the parameter $R$ defined in @eq:R. This represents the length of the diagonal connecting one vertex of the parallelepiped to its opposite vertex. Now, imagine placing the solid inside a sphere of radius RR, with one vertex positioned at the origin and the opposite vertex touching the surface of the sphere, as illustrated in the following figure:

#figure(
  image("../images/space_sphere.png", width: 60%),
  caption: [Sample of a parallelepiped solid inside a sphere.]
)<fig:aspect_ratio_space> 

Here, one can clearly see that the dimensions of the solid correspond to the coordinates of the vertex opposite the origin, where:

$ L_x = R sin(theta) cos(phi), " " L_y = R sin(theta) sin(phi), " " L_z = R cos(theta). $

The angles $theta$ and $phi$ can represent any point on the surface of the sphere, and thus they can also represent any aspect ratio of the solid. For example, the aspect ratio 3:4:5 can be represented with the following angles: $theta = arctan(sqrt(3^2 + 4^2)/5) = arctan(1) = pi/4$, $phi = arctan(3/4) = 0.2 pi$. Since physical solids cannot have negative lengths, only angles in the range $[0, pi/2]$ are valid. Therefore, we can represent all valid aspect ratios by generating values of $theta$ and $phi$ within this range.  

However, there is redundancy in this representation. For instance, a solid with dimensions $L_x = 5$, $L_y = 4$, and $L_z = 3$ will have the same eigenvalues as a solid with $L_x = 4$, $L_y = 5$, and $L_z = 3$, yet these configurations correspond to different values of $theta$ and $phi$. This redundancy must be addressed.

Lets define a new set of angles in a transformed coordinate space, which will allow us to more easily identify and eliminate symmetrical configurations. The first of these will be our new polar angle, denoted by $eta$, and defined as:

$ eta = 2 theta, $<eq:eta_definition>

and the second will be our new azimuthal angle, denoted by $beta$, and defined as:

$ beta = 4 phi. $<eq:beta_definition>

With this transformation, $eta$ ranges from 0 to $pi$, and $beta$ ranges from 0 to $2 pi$. The sample dimensions can now be expressed in terms of these new angles as follows: 

$ L_x = R sin(1/2 eta) cos(1/4 beta), " " L_y = R sin(1/2 eta) sin(1/4 beta), " " L_z = R cos(1/2 eta). $<eq:dim_in_terms_of_eta_and_beta>

These transformed angles yield values of $L_x$, $L_y$, and $L_z$ that are always positive, allowing us to map all valid configurations of the solid within the surface of the new sphere, illustrated in @fig:space_sphere_eta.

For example:

- A value of $eta = pi/2$ (i.e., a point on the equator of the new sphere) corresponds to the case where $L_z = sqrt(L_x^2 + L_y^2)$.
- A value of $eta$ close to 0 implies that $L_z >> sqrt(L_x^2 + L_y^2)$.
- A value of $eta$ close to $pi$ implies that $L_z << sqrt(L_x^2 + L_y^2)$.

Similarly, the angle $beta$ describes the relative magnitudes of $L_x$ and $L_y$:

- $beta = pi$ corresponds to $L_x = L_y$,
- A value of $beta$ close to 0 indicates that $L_x >> L_y$,
- A value of $beta$ close to $2 pi$ indicates that $L_x << L_y$.

To start reducing redundancy, we assign the largest dimension to $L_z$ and the smallest to $L_y$. Under this convention, all possible aspect ratios can be represented using values of:

- $eta$ between 0 and approximately $0.61 pi$ (this upper bound corresponds to the case $L_x = L_y = L_z$),

- and $beta$ between 0 and $pi$.

Although some residual redundancy still exists — for example, at $beta = 0$, all points with $eta > pi/2$ have equivalent points in the region $eta < pi/2$ — this transformation significantly reduces the number of symmetric or duplicate configurations. As a result, we can now generate data that effectively represents all valid aspect ratios of a parallelepiped sample.

// Figure of sphere in eta, beta space. This one can be 2D.
#figure(
  image("../images/space_sphere_eta.png", width: 45%),
  caption: [Visual representation of the aspect ratio of the sample. $eta_1$ represents a solid with a shape similar to a match stick, while $eta_2$ represents a sample shaped like a PC case and $eta_3$ represents a sample shaped like a pizza box]
)<fig:space_sphere_eta>

In summary, the angles $eta$ and $beta$ can represent every possible aspect ratio of a sample, and the eigenvalues $lambda_n$ depend only on the aspect ratio — not on the absolute size of the sample. Therefore, the eigenvalues $lambda_n$ are functions of $eta$ and $beta$, but not of $R$.

As a result, when training the machine learning model, we can reduce the number of geometric input features from three ($L_x$, $L_y$, and $L_z$) to just two: $eta$ and $beta$. These new features can be computed from the dimensions using the following expressions:

$ eta = 2 arctan(sqrt(L_x^2 + L_y^2)/L_z), $<eq:def_eta>

and 

$ beta = 4 arctan(L_y/L_x). $<eq:def_beta> 

== Setting new targets according to the elastic constant restrictions

One of the main challenges in building a model to solve the inverse problem lies in deciding how to generate the elastic constants that will form the training data. As discussed in @chap:failure, some values were previously selected based on a literature review. However, this approach is problematic, as there is no guarantee that a model trained on such values will accurately predict elastic constants that differ significantly from those found in the literature.

Just as we did with the geometric dimensions, we want the model to work with any valid constant value, regardless of its order of magnitude. In the following sections, we will show that it is possible to train a machine learning model to learn the relationships between the constants ($G/K$ in the isotropic case, for example), and then recover their absolute magnitudes through simple post-processing — specifically, by applying a small number of rescaling transformations.

=== Setting new targets for the isotropic case

Consider an isotropic solid, which we’ll call solid A, with a bulk modulus of $K_A = 400 "GPa"$ and a shear modulus of $G_A = 300 "GPa"$, along with some given dimensions. We then perform a forward problem using @eq:Peso_matrix_def to compute its eigenvalues: $lambda_(A 0)$, $lambda_(A 1)$, $lambda_(A 2)$, ..., $lambda_(A n)$. Now consider a different isotropic solid, solid B, with the same shape and dimensions, but with elastic constants doubled: $K_B = 800 "GPa"$ and $G_B = 600 "GPa"$. Performing the forward problem for solid B yields the eigenvalues: 

$ lambda_(B 0) = 2 lambda_(A 0), " "lambda_(B 1) = 2 lambda_(A 1), " "lambda_(B 2) = 2 lambda_(A 2), ..., " "lambda_(B n) = 2 lambda_(A n). $ 

This happens because of the linear dependence of @eq:Peso_matrix_def on the elastic constants. If we scale all elastic constants by a factor, while keeping their ratios fixed, each eigenvalue scales by the same factor. This implies that the eigenvalues are proportional to the overall magnitude of the constants.

We define this magnitude as:

$ M = sqrt(K^2 + G^2), $

although other definitions (e.g., $M = K +G$) could be used. However, for reasons similar to those outlined in @sec:eta_beta_definition, the root-sum-of-squares form is more practical in this context, as we will see later. Thus, for a fixed ratio between $K$ and $G$, the eigenvalues satisfy the proportionality:

$ lambda_n prop M, $<eq:prop_M_lambda>

which holds regardless of the absolute scale of the elastic constants. 

Now let us define a new variable, $xi$, which captures a relationship between consecutive eigenvalues:

$ xi_n = lambda_n/lambda_(n+1). $

The values of $xi_n$ are the same for both solid A and solid B. This is because $xi_n$ depends only on the ratio between $K$ and $G$, not on their absolute magnitudes. In other words, we have constructed a new variable that captures the elastic behavior independent of scale.

$xi_n$ is not the only variable capable of expressing the relationship between eigenvalues. For example, consider:

$ chi_n = (lambda_n - lambda_(n-1))/lambda_n, $

which will be used in the cubic case later. This variable also depends solely on the relationship between the elastic constants and the aspect ratio of the sample.

As we saw in @sec:eta_beta_definition, one effective way to encode the relationship between variables in a bounded and continuous way is through angles. In that spirit, we now define a new variable that encodes the ratio between $K$ and $G$, which we will call $phi_K$:

$ phi_K = arctan(G/K). $<eq:phiK_def_isotropic>

Although variables such as $G/K$ or $K/(K+G)$ can be used to represent the relationship between $K$ and $G$, each has drawbacks. The ratio $K/G$ can grow arbitrarily large, making it computationally impractical when generating training data. The expression $K/(K+G)$ has a bounded range between 0 and 1 and can be interpreted as a composition, but in the cubic case, it suffers from the same limitations we encountered with the $g$ values discussed in @sec:eta_beta_definition.

By contrast, the angle $phi_K$, is defined over the finite interval $[0, pi/2]$, making it a more robust and practical choice as a target variable for a machine learning model. It can be easily sampled within a bounded and computationally safe range, and a similar angular variable can be defined for the cubic case. 

Once we have a model capable of predicting $phi_K$ based on the eigenvalue relationships and the geometric features $eta$ and $beta$, we still need to recover the absolute values of $K$ and $G$. To do this, we must compute the magnitude $sqrt(K^2 + G^2)$, and then use it along with $phi_K$ to retrieve both constants. Fortunately, this process is straightforward and can be accomplished through the following steps:

- Predict $phi_K$ using the features $xi_n$, $eta$, and $beta$.
Establish an arbitrary base magnitude — for example, set: $M_"base" = sqrt(K_("base")^2 + G_("base")^2) = 1$, and compute base values of the elastic constants using:
$ K_("base") = M_"base" cos(phi_K), " "G_("base") = M_"base" sin(phi_K). $

- Compute the eigenvalues corresponding to the "base" constants by solving the forward problem. Let these be denoted as $lambda_0^"base", lambda_1^"base", dots$.
  Note that the relationships between these eigenvalues — expressed through $xi_n^"base"$ — must match those of the original data ($xi_n$).

- Determine the true magnitude $M = sqrt(K^2 + G^2)$ using the proportional relationship established in @eq:prop_M_lambda. This can be done using any eigenvalue (not necessarily $lambda_0$):

$ M = (lambda_0/lambda_0^"base") M_"base" = sqrt(K^2 + G^2) = (lambda_0/lambda_0^("base")) sqrt(K_("base")^2 + G_("base")^2). $<eq:M_determination1>

- Finally, compute the elastic constants $K$ and $G$ using the recovered magnitude $M$ and the predicted angle $phi_K$:

$ K = M cos(phi_K), $<eq:isotropic_K_relation> 
$ G = M sin(phi_K). $<eq:isotropic_G_relation>

In summary, the original problem of training a model to predict the elastic constants $K$ and $G$ from the resonance frequencies $omega_n$, the dimensions $L_x$, $L_y$, $L_z$, and the density $rho$ has been reformulated into a simpler, rescaled and more robust task: predicting the angle $phi_K$ from the eigenvalue relationships $xi_n$ and the geometric parameters $eta$ and $beta$.

=== Setting new targets in cubic case<sec:cubic_targets>

In the cubic case, the elastic constant matrix takes the form shown in @eq:cubic_constant_matrix. To perform a target transformation similar to that used in the isotropic case—expressing the elastic constants in terms of angles—we must identify variables that are strictly positive, just as $K$ and $G$ were in the isotropic formulation.

Fortunately, the thermodynamic stability conditions provided by Mouhat and Coudert #cite(<Mouhat_2014>) (and discussed in detail in @section:Constant_Restrictions) allow us to define new transformed variables that are constrained to be positive by construction. These new variables serve as the basis for our transformation.

Let us now define the new variables, based on the stability conditions described in @eq:restrictions_cubic_solids:

$ Kappa = 1/3 (C_11 + 2C_12); " "a = 1/3(C_11 - C_12); " "mu = C_44. $<eq:cubic_const_tranf>

Each of these variables corresponds to a stability condition from @eq:restrictions_cubic_solids, redefined here as standalone positive quantities. The factor of $(1/3)$ was introduced to make the resulting elastic constant matrix more directly comparable to the isotropic case. With these new variables, the elastic constant matrix for a cubic solid can be rewritten as follows:

$ arrow.l.r(C) = mat(
  Kappa + 2a, Kappa - a, Kappa - a, 0, 0, 0;
  Kappa - a, Kappa + 2a, Kappa - a, 0, 0, 0;
  Kappa - a, Kappa - a, Kappa + 2a, 0, 0, 0;
  0, 0, 0, mu, 0, 0;
  0, 0, 0, 0, mu, 0;
  0, 0, 0, 0, 0, mu;
) $<eq:cubic_constant_matrix_definitive>

Each of the new variables, have now the following restrictions: 

$ Kappa > 0; a > 0; mu >0. $

Just like the isotropic case, here the eigenvalues are also proportional to the magnitude of the constants. For example the eigenvalues of a solid with values of $Kappa = 5$, $a = 4$ and $mu = 3$ will be half the eigenvalues of a solid, with the same dimensions, with values of $Kappa = 10$, $a = 8$ and $mu = 6$. Thus the following applies to cubic solids: 

$ lambda_n prop M = sqrt(Kappa^2 + a^2 + mu^2), $<eq:prop_M_lambda2>

for a given relation between $Kappa$, $a$ and $mu$. Also in this case the relations between eigenvalues depend only on the relations between constants and the aspect ratio of the sample. The relations between constants will be represented here with the following angles: 

$ phi_Kappa = arctan(sqrt(a^2 + mu^2)/Kappa), $

and, 

$ phi_a = arctan(mu/a). $

In the case of cubic solids, the relation between eigenvalues will be represented only by the variable 

$ chi_n = (lambda_n - lambda_(n-1))/lambda_N, $<eq:chi_definition>

where $N$ is the maximum number of eigenvalues used, because to train a model able to predict $phi_a$ and $phi_Kappa$ we will need to use more eigenvalues. To be exact $N = 20$ eigenvalues. This makes $xi_n$ a bad candidate for use as a feature, because for large $n$, like $n = 20$, all the values of $xi_n$ will be very close to 1. In the present work the variables $chi_n$ were given a special name: "compositions". This is because $chi_n$ represents the percentage of occupation of the gap between two eigenvalues in the whole spectrum from 0 to the Nth eigenvalue (20th in this case). The first composition $chi_0$ is defined differently as the others: $chi_0 = lambda_0/lambda_N$. Similar to the isotropic case we can create a model able to predict $phi_Kappa$ and $phi_a$ given the values of $chi_n$, $eta$ and $beta$. Once we have that model, we perform the following steps to get $Kappa$, $a$ and $mu$:

- Predict $phi_Kappa$ and $phi_a$ using the values of $chi_n$, $eta$ and $beta$ as the features. 
- Establish an arbitrary base value of magnitude, for example $M = sqrt(Kappa_("base")^2 + a_("base")^2 + mu_("base")^2) = 1$ and calculate a base value of $Kappa$, $a$ and $mu$ using: 
$ Kappa_("base") = M cos(phi_Kappa), $
$ a_("base") = M sin(phi_Kappa)cos(phi_a), $
$ mu_("base") = M sin(phi_Kappa)sin(phi_a). $
- Compute the eigenvalues of the "base" constants performing a forward problem. Lets call them $lambda_0^("fwd"), lambda_1^("fwd")$, etc. Note that the relation between eigenvalues obtained from this forward problem $chi_n^("fwd")$ must be equal to the original eigenvalues $chi_n$.
- Get the real magnitude $sqrt(Kappa^2 + a^2 + mu^2)$, using the proportion relation mentioned in @eq:prop_M_lambda2, with any of the eigenvalues (not necessarily $lambda_0$): $ sqrt(Kappa^2 + a^2 + mu^2) = (lambda_0/lambda_0^("fwd")) sqrt(Kappa_("base")^2 + a_("base")^2 + mu_("base")^2). $<eq:M_determination_cubic>
- Finally, get the constants $Kappa$, $a$ and $mu$ the following way: 
$ Kappa = M cos(phi_K), $<eq:cubic_K_relation> 
$ a = M sin(phi_Kappa)cos(phi_a), $<eq:cubic_a_relation>
$ mu = M sin(phi_Kappa)sin(phi_a). $<eq:cubic_mu_relation>

All the transformations of features and targets, or in general, the transformation of the inverse problem can be summarized in the following flow diagram: 

#figure(
  image("../images/Inverse_diagram.png", width: 100%),
  caption: [Summary of the parts that make up the approach of the inverse problem in the present work.]  
) <fig:diagram_inverse>


